mod commands;
mod network_events_listener;
mod llm_proxy_client;
mod reincarnation;
pub mod usage_verification;
pub(crate) mod memories;
pub(crate) mod container_manager;

pub use commands::NodeCommand;
pub use container_manager::{ContainerManager, RestartReason};
use futures::future::ok;

use std::collections::{HashMap, HashSet};
use std::str::Split;
use std::sync::Arc;
use std::pin::Pin;
use std::fs;
use color_eyre::{Result, eyre::anyhow, eyre::Error};
use colored::Colorize;
use futures::FutureExt;
use futures::pin_mut;
use hex;
use libp2p::{core::Multiaddr, PeerId};
use tokio::sync::{mpsc, oneshot};
use tokio::sync::RwLock;
use tracing::{info, debug, error, warn};
use rand::seq::SliceRandom;
use rand::thread_rng;
use sha3::{Digest, Keccak256};
use serde::{Serialize, Deserialize, de::DeserializeOwned};
use p256::ecdsa::{
    VerifyingKey as P256VerifyingKey,
    Signature as P256Signature
};
use ecdsa::signature::Verifier;
use elliptic_curve::pkcs8::DecodePublicKey;
use base64::{Engine as _, engine::general_purpose::STANDARD as base64_standard};

use crate::{get_node_name, short_peer_id, TryPeerId};
use crate::network_events::NodeIdentity;
use crate::types::{
    ReverieNameWithNonce,
    NetworkEvent,
    NodeKeysWithVesselStatus,
    RespawnId,
    Reverie,
    ReverieCapsulefrag,
    ReverieId,
    ReverieKeyfrag,
    ReverieKeyfragMessage,
    ReverieMessage,
    ReverieType,
    VesselStatus,
    AccessCondition,
    AccessKey,
};
use crate::SendError;
use crate::behaviour::heartbeat_behaviour::TeePayloadOutEvent;
use crate::node_client::usage_verification::verify_usage_report;
use crate::usage_db::{UsageDbPool, store_usage_payload};
use crate::env_var::EnvVars;

use runtime::reencrypt::{UmbralKey, VerifiedCapsuleFrag};
use runtime::llm::AgentSecretsJson;
use llm_proxy::usage::SignedUsageReport;
use crate::utils::pubkeys::encode_libp2p_pubkey_to_pem;
use std::path::Path;
use std::time::Duration;
use tokio::time::timeout as tokio_timeout;
use libp2p::identity::Keypair as IdentityKeypair;
use nanoid;
use std::process::Command;
use regex::Regex;

// Define a simple error type for NodeClient operations, can be expanded
#[derive(Debug)]
pub enum NodeClientError {
    CommandSendError(String),
    ResponseReceiveError(String),
    Other(String), // For miscellaneous errors
}

impl std::fmt::Display for NodeClientError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            NodeClientError::CommandSendError(s) => write!(f, "CommandSendError: {}", s),
            NodeClientError::ResponseReceiveError(s) => write!(f, "ResponseReceiveError: {}", s),
            NodeClientError::Other(s) => write!(f, "NodeClientError: {}", s),
        }
    }
}

impl std::error::Error for NodeClientError {}

// Helper to convert from SendError (if that's what your other methods might return)
impl From<SendError> for NodeClientError {
    fn from(err: SendError) -> Self {
        NodeClientError::Other(err.to_string())
    }
}

#[derive(Clone)]
pub struct NodeClient {
    pub node_id: NodeIdentity,
    pub command_sender: mpsc::Sender<NodeCommand>,
    // hb subscriptions for rpc clients
    pub heartbeat_receiver: async_channel::Receiver<TeePayloadOutEvent>,
    // keep private in TEE
    umbral_key: UmbralKey,
    // Proxy's public key for verifying usage reports
    pub llm_proxy_public_key: Arc<RwLock<Option<P256VerifyingKey>>>,
    // Proxy's Hudsucker CA certificate PEM for establishing TLS connections with llm-proxy
    pub llm_proxy_ca_cert: Arc<RwLock<Option<reqwest::Certificate>>>,
    pub usage_db_pool: UsageDbPool,
}

impl NodeClient {
    pub fn new(
        node_id: NodeIdentity,
        command_sender: mpsc::Sender<NodeCommand>,
        umbral_key: UmbralKey,
        heartbeat_receiver: async_channel::Receiver<TeePayloadOutEvent>,
        usage_db_pool: UsageDbPool,
    ) -> Self {
        Self {
            node_id,
            command_sender,
            heartbeat_receiver,
            umbral_key,
            llm_proxy_public_key: Arc::new(RwLock::new(None)),
            llm_proxy_ca_cert: Arc::new(RwLock::new(None)),
            usage_db_pool,
        }
    }

    pub fn create_reverie<T: Serialize + DeserializeOwned>(
        &mut self,
        secrets: T,
        reverie_type: ReverieType,
        threshold: usize,
        total_frags: usize,
        target_public_key: umbral_pre::PublicKey,
        verifying_public_key: umbral_pre::PublicKey,
        access_condition: AccessCondition,
    ) -> Result<Reverie> {

        let (
            capsule,
            ciphertext
        ) = self.umbral_key.encrypt_bytes(&serde_json::to_vec(&secrets)?)?;

        // secrets description: generated by an LLM who looks at the secrets/MCP/executable
        let reverie = Reverie::new(
            "secrets description".to_string(),
            reverie_type,
            threshold,
            total_frags,
            target_public_key,
            verifying_public_key,
            access_condition,
            capsule,
            ciphertext
        );

        Ok(reverie)
    }

    pub fn create_reverie_keyfrags(
        &self,
        reverie: &Reverie,
    ) -> Result<Vec<ReverieKeyfrag>> {
        // Alice generates reencryption key fragments for MPC nodes
        info!("Generating {}-of-{} keyfrags for reverie: {}", reverie.threshold, reverie.total_frags, reverie.id);

        let kfrags = self.umbral_key.generate_pre_keyfrags(
            &reverie.target_public_key,
            reverie.threshold,
            reverie.total_frags,
            true, // verify kfrag corresponds to a given delegating pubkey
            false, // verify kfrag belongs to a given target pubkey
        ).iter().enumerate().map(|(i, kfrag)| {
            ReverieKeyfrag {
                id: reverie.id.clone(),
                reverie_type: reverie.reverie_type.clone(),
                frag_num: i,
                threshold: reverie.threshold,
                total_frags: reverie.total_frags,
                umbral_keyfrag: serde_json::to_vec(&kfrag).expect(""),
                umbral_capsule: reverie.umbral_capsule.clone(),
                source_pubkey: self.umbral_key.public_key,
                target_pubkey: reverie.target_public_key,
                source_verifying_pubkey: self.umbral_key.verifying_public_key,
                target_verifying_pubkey: reverie.verifying_public_key,
                access_condition: reverie.access_condition.clone(),
            }
        }).collect::<Vec<ReverieKeyfrag>>();

        Ok(kfrags)
    }

    pub async fn get_prospect_vessels(
        &self,
        shuffle: bool
    ) -> Result<(NodeKeysWithVesselStatus, Vec<NodeKeysWithVesselStatus>)> {

        let peer_nodes = self.get_node_vessels(shuffle).await
            .into_iter()
            .filter(|v| v.vessel_status == VesselStatus::EmptyVessel)
            .collect::<Vec<NodeKeysWithVesselStatus>>();

        let (
            target_vessel,
            target_kfrag_providers
        ) = peer_nodes
            .split_first()
            .ok_or(anyhow!("No Peers found."))?;

        if target_kfrag_providers.contains(&target_vessel) {
            return Err(anyhow!("Target vessel cannot also be a kfrag provider"));
        }

        Ok((target_vessel.clone(), target_kfrag_providers.to_vec()))
    }

    pub async fn broadcast_reverie_keyfrags(
        &mut self,
        reverie: &Reverie,
        target_vessel_peer_id: PeerId,
        mut target_kfrag_providers: Vec<NodeKeysWithVesselStatus>
    ) -> Result<(), SendError> {

        let umbral_ciphertext = reverie.umbral_ciphertext.clone();

        // Split into fragments
        let kfrags = self.create_reverie_keyfrags(&reverie)?;

        if target_kfrag_providers.len() < reverie.total_frags {
            return Err(SendError(format!(
                "Not connected to enough peers, need: {}, got: {}",
                reverie.total_frags + 1,
                target_kfrag_providers.len() + 1
            )));
        } else if target_kfrag_providers.len() > reverie.total_frags {
            target_kfrag_providers = target_kfrag_providers
                .into_iter()
                .take(reverie.total_frags)
                .collect();
        }
        info!("Kfrag providers: {}", target_kfrag_providers.len());
        info!("Total frags: {}", reverie.total_frags);

        // Create futures for broadcasting Kfrags to peer nodes
        let send_kfrag_futures = futures::future::try_join_all(
            kfrags.into_iter().enumerate().map(|(i, reverie_keyfrag)| {
                let keyfrag_provider = target_kfrag_providers[i].peer_id;
                let source_peer_id = self.node_id.peer_id.clone();
                self.command_sender.send(
                    NodeCommand::SendReverieKeyfrag {
                        keyfrag_provider: keyfrag_provider,
                        reverie_keyfrag_msg: ReverieKeyfragMessage {
                            reverie_keyfrag: reverie_keyfrag,
                            source_peer_id: source_peer_id,
                            target_peer_id: target_vessel_peer_id,
                        },
                    }
                )
            })
        );

        // Add Memory trading:
        // make it easy for a user ALICE (not running a node) to
        // - submit a memory + PRE it for BOB (not running a node)
        // - BOB to decrypt and run the memory as context for a TEE Agent

        // Expand memories to MCP plugins
        // - Alice can now upload MCP plugins with API keys and rent access to them to BOB
        // - Alice can even delegate access to entire github repos to execute in TEEs to BOB
        // - Problem is marketing the MCP plugins: how does BOB know what the plugin does without seeing the code?
        // - Use an AI description of the plugin? document endpoints?

        match reverie.reverie_type {
            ReverieType::SovereignAgent(..) => {
                // Send Ciphertext to target vessel if ReverieType::SovereignAgent
                let (f1, f2) = futures::future::join(
                    // Send all Kfrags
                    send_kfrag_futures,
                    // Send Reverie to the target vessel
                    self.command_sender.send(
                        NodeCommand::SendReverieToSpecificPeer {
                            ciphertext_holder: target_vessel_peer_id, // Ciphertext Holder
                            reverie_msg: ReverieMessage {
                                reverie: reverie.clone(),
                                source_peer_id: self.node_id.peer_id,
                                target_peer_id: target_vessel_peer_id,
                                keyfrag_providers: target_kfrag_providers.iter()
                                    .map(|v| v.peer_id).collect(),
                            },
                        }
                    )
                ).await;
                // ensure f1, f2 both return Ok(())
                f1.and(f2).map_err(SendError::from)
            }
            _ => {
                let (f1, f2, f3) = futures::future::join3(
                    // Send all Kfrags
                    send_kfrag_futures,
                    // Save Reverie on DHT for other ReverieTypes (Agent, Memory, Retrieval, Tools)
                    self.command_sender.send(
                        NodeCommand::SaveReverieOnNetwork {
                            reverie_msg: ReverieMessage {
                                reverie: reverie.clone(),
                                source_peer_id: self.node_id.peer_id,
                                target_peer_id: target_vessel_peer_id,
                                keyfrag_providers: target_kfrag_providers.iter()
                                    .map(|v| v.peer_id).collect(),
                            },
                        }
                    ),
                    // Still need to send to the target vessel
                    // so that it can track prev_vessel node for failure/respawning.
                    self.command_sender.send(
                        NodeCommand::SendReverieToSpecificPeer {
                            ciphertext_holder: target_vessel_peer_id, // Ciphertext Holder
                            reverie_msg: ReverieMessage {
                                reverie: reverie.clone(),
                                source_peer_id: self.node_id.peer_id,
                                target_peer_id: target_vessel_peer_id,
                                keyfrag_providers: target_kfrag_providers.iter()
                                    .map(|v| v.peer_id).collect(),
                            },
                        }
                    )
                ).await;
                // ensure f1, f2, f3 all return Ok(())
                f1.and(f2).and(f3).map_err(SendError::from)
            }
        }
    }

    pub async fn get_node_vessels(&self, shuffle: bool) -> Vec<NodeKeysWithVesselStatus> {
        let (sender, mut receiver) = mpsc::channel(100);
        self.command_sender
            .send(NodeCommand::GetNodeVesselStatusesFromKademlia { sender })
            .await
            .expect("Command receiver not to be dropped.");

        let mut pks: Vec<NodeKeysWithVesselStatus> = vec![];
        while let Some(pk) = receiver.recv().await {
            debug!("Received Peer Umbral PK => {}", pk);
            pks.push(pk);
        }

        if shuffle {
            let mut rng = thread_rng();
            pks.shuffle(&mut rng);
        }

        pks
    }

    pub async fn get_reverie(&self, reverie_id: &ReverieId, reverie_type: ReverieType) -> Result<ReverieMessage> {
        let (sender, receiver) = oneshot::channel();

        self.command_sender
            .send(NodeCommand::GetReverie {
                reverie_id: reverie_id.clone(),
                reverie_type: reverie_type,
                sender: sender
            })
            .await?;

        receiver.await.map_err(SendError::from)?
    }

    pub async fn request_cfrags(
        &mut self,
        reverie_id: &ReverieId,
        keyfrag_providers: Vec<PeerId>,
        access_key: AccessKey
    ) -> Vec<Result<Vec<u8>, SendError>> {

        let requests = keyfrag_providers.iter()
            .map(|kfrag_provider_peer_id| {

                let reverie_id2 = reverie_id.clone();
                let access_key2 = access_key.clone();
                let nc = self.clone();

                info!("Requesting {} cfrag from {}", &reverie_id2, get_node_name(&kfrag_provider_peer_id));
                // Request key fragment from each node that holds that fragment.
                async move {
                    let (sender, receiver) = oneshot::channel();
                    nc.command_sender
                        .send(NodeCommand::RequestCapsuleFragment {
                            reverie_id: reverie_id2.clone(),
                            kfrag_provider_peer_id: kfrag_provider_peer_id.clone(),
                            access_key: access_key2,
                            sender
                        })
                        .await?;

                    receiver.await.map_err(|e| anyhow!(e.to_string()))
                }.boxed()
            });

        if let Ok(cfrags) = futures::future::try_join_all(requests).await {
            cfrags
        } else {
            vec![]
        }
    }

    fn parse_cfrags(
        &self,
        cfrags_raw: Vec<Result<Vec<u8>, SendError>>,
        capsule: umbral_pre::Capsule,
    ) -> Result<(
        Vec<VerifiedCapsuleFrag>,
        umbral_pre::PublicKey,
        umbral_pre::PublicKey,
        AccessCondition,
        usize
    )> {

        let mut verified_cfrags: Vec<VerifiedCapsuleFrag> = Vec::new();
        let mut reverie_cfrags: Vec<ReverieCapsulefrag> = Vec::new();

        let mut required_threshold = 0;
        let mut total_frags_received = 0;

        for cfrag_result in cfrags_raw.into_iter() {

            // Deserialize capsule fragments
            let reverie_cfrag: ReverieCapsulefrag = serde_json::from_slice(&cfrag_result?)?;
            let cfrag = reverie_cfrag.encode_capsule_frag()?;
            let new_vessel_pubkey  = reverie_cfrag.target_pubkey;

            info!("Success! cfrag({}) from {}\ntotal frags: {}",
                reverie_cfrag.frag_num,
                get_node_name(&reverie_cfrag.kfrag_provider_peer_id),
                total_frags_received
            );

            // Target vessel must check that cfrags are valid.
            let verified_cfrag = cfrag.verify(
                &capsule,
                &reverie_cfrag.source_verifying_pubkey, // verifying pk
                &reverie_cfrag.source_pubkey, // source pubkey
                &reverie_cfrag.target_pubkey // target pubkey
            ).map_err(|(e, _)| anyhow!(e.to_string()))?;

            total_frags_received += 1;
            required_threshold = reverie_cfrag.threshold;
            verified_cfrags.push(verified_cfrag);
            reverie_cfrags.push(reverie_cfrag);
        }

        info!("Received {}/{} required CapsuleFrags", total_frags_received, required_threshold);

        if total_frags_received < required_threshold {
            warn!("Not enough fragments. Need {required_threshold}, received {total_frags_received}");
            return Err(anyhow!("Insufficient cfrags fragments received"))
        }

        // delegator pubkey
        let first_cfrag = match reverie_cfrags.iter().next() {
            Some(cfrag) => cfrag.clone(),
            None => return Err(anyhow!("No cfrags received"))
        };

        // Validate that all capsule fragments are from the same Reverie
        let valid_cfrags = reverie_cfrags.iter().all(|cfrag| {
            cfrag.source_pubkey == first_cfrag.source_pubkey &&
            cfrag.target_pubkey == first_cfrag.target_pubkey &&
            cfrag.source_verifying_pubkey == first_cfrag.source_verifying_pubkey &&
            cfrag.target_verifying_pubkey == first_cfrag.target_verifying_pubkey
        });


        Ok((
            verified_cfrags,
            first_cfrag.source_pubkey,
            first_cfrag.target_verifying_pubkey,
            first_cfrag.access_condition,
            total_frags_received
        ))
    }

    fn decrypt_cfrags<T: Serialize + DeserializeOwned>(
        &self,
        capsule: umbral_pre::Capsule,
        ciphertext: Box<[u8]>,
        source_pubkey: umbral_pre::PublicKey,
        verified_cfrags: Vec<VerifiedCapsuleFrag>,
    ) -> Result<T, Error> {

        // Bob (next target vessel) uses his umbral_key to open the capsule by using at
        // least threshold cfrags, then decrypts the re-encrypted ciphertext.
        match self.umbral_key.decrypt_reencrypted(
            &source_pubkey, // delegator pubkey
            &capsule, // capsule,
            verified_cfrags, // verified capsule fragments
            ciphertext // ciphertext
        ) {
            Ok(plaintext_bob) => {
                let decrypted_data: serde_json::Value = serde_json::from_slice(&plaintext_bob)?;
                let secrets_str = serde_json::to_string_pretty(&decrypted_data)?;
                println!("Decrypted (re-encrypted) secrets:\n{}", format!("{}", secrets_str).bright_black());
                let secrets_json = serde_json::from_slice(&plaintext_bob)?;
                Ok(secrets_json)
            },
            Err(e) => {
                error!("{}", e);
                warn!("Not decryptable by user {} with: {}", self.node_id.node_name, self.umbral_key.public_key);
                warn!("Target decryptor pubkey: {}", source_pubkey);
                Err(anyhow!(e.to_string()))
            }
        }
    }

    fn nname(&self) -> String {
        format!("{}{}", self.node_id.node_name.yellow(), ">".blue())
    }

    pub async fn get_reverie_id_by_name(&self, reverie_name_nonce: &ReverieNameWithNonce) -> Option<ReverieId> {
        let (sender, receiver) = tokio::sync::oneshot::channel();
        self.command_sender
            .send(NodeCommand::GetReverieIdByName {
                reverie_name_nonce: reverie_name_nonce.clone(),
                sender: sender,
            })
            .await.expect("Command receiver not to bee dropped");

        receiver.await.expect("get reverie receiver not to drop")
    }

    pub async fn simulate_node_failure(&mut self) -> Result<RestartReason> {
        let (sender, receiver) = oneshot::channel();
        self.command_sender.send(NodeCommand::SimulateNodeFailure {
            sender,
            reason: RestartReason::NetworkHeartbeatFailure,
        }).await.ok();

        receiver.await.map_err(|e| anyhow!(e.to_string()))
    }

    pub async fn get_node_state(&self) -> Result<serde_json::Value> {

        let (sender, receiver) = oneshot::channel();
        self.command_sender.send(NodeCommand::GetNodeState {
            sender: sender,
        }).await.ok();

        let node_info = receiver.await
            .map_err(|e| anyhow!(e.to_string()))?;

        Ok(node_info)
    }

    pub fn get_hb_channel(&self) -> async_channel::Receiver<TeePayloadOutEvent> {
        self.heartbeat_receiver.clone()
    }

    /// Verifies and potentially stores a usage report received from a proxy.
    /// TODO: move to network_events and dispatch on-chain
    pub async fn report_usage(&mut self, signed_usage_report: SignedUsageReport) -> Result<String> {
        info!("Received usage report: Payload(first 50)={:?}, Signature(first 10)={:?}",
              &signed_usage_report.payload[..signed_usage_report.payload.len().min(50)],
              &signed_usage_report.signature[..signed_usage_report.signature.len().min(10)]);

        // Get the public key from self.proxy_public_key
        let proxy_public_key = self.llm_proxy_public_key.read().await;
        let opt_public_key = proxy_public_key.as_ref();
        let public_key = match opt_public_key {
            Some(ref key) => key,
            None => {
                error!("NodeClient: LLM Proxy public key not available for usage report verification. Has it registered yet?");
                return Err(anyhow!("LLM Proxy public key not available. Cannot verify usage report."));
            }
        };

        match verify_usage_report(&signed_usage_report, public_key) { // Pass the borrowed key
            Ok(payload) => {
                info!("NodeClient: Usage report verified successfully.");
                // Verification successful, now store the payload
                if let Err(db_err) = store_usage_payload(&self.usage_db_pool, &payload) {
                    error!("NodeClient: Failed to store verified usage payload in DB: {}", db_err);
                }
                Ok("Usage report verified.".to_string())
            }
            Err(verification_error) => {
                error!("NodeClient: Usage report verification failed: {}", verification_error);
                Err(anyhow!("Usage report verification failed: {}", verification_error))
            }
        }
    }

    pub async fn get_connected_peers(&self) -> Result<Vec<PeerId>, NodeClientError> {
        let (tx, rx) = oneshot::channel();
        self.command_sender
            .send(NodeCommand::GetConnectedPeers { responder: tx })
            .await
            .map_err(|e| NodeClientError::CommandSendError(e.to_string()))?;

        rx.await.map_err(|e| NodeClientError::ResponseReceiveError(e.to_string()))
    }

    pub async fn register_llm_proxy_pubkey(
        &mut self,
        payload: llm_proxy::types::LlmProxyPublicKeyPayload,
    ) -> Result<String, Error> {
        info!("NodeClient: Processing LLM Proxy public key registration...");

        // 1. Parse the PEM public key string to a P256VerifyingKey
        let llm_proxy_pubkey = P256VerifyingKey::from_public_key_pem(&payload.pubkey_pem)
            .map_err(|e| anyhow!("Failed to parse LLM Proxy public key PEM: {}", e))?;
        debug!("NodeClient: Successfully parsed LLM Proxy public key PEM.");

        // 2. Decode the Base64 signature
        let signature_bytes = base64_standard.decode(&payload.signature_b64)
            .map_err(|e| anyhow!("Failed to decode Base64 signature for LLM Proxy key: {}", e))?;
        let signature = P256Signature::from_slice(&signature_bytes)
            .map_err(|e| anyhow!("Failed to parse signature bytes for LLM Proxy key: {}", e))?;
        debug!("NodeClient: Successfully decoded and parsed signature.");

        // 3. Verify the signature
        // The signature was made over the PEM string of the public key itself.
        llm_proxy_pubkey.verify(payload.pubkey_pem.as_bytes(), &signature)
            .map_err(|e| anyhow!("LLM Proxy public key signature verification failed: {}", e))?;
        println!("NodeClient: LLM Proxy public key signature VERIFIED.");

        // 4. Store the verified public key
        let _ = self.llm_proxy_public_key.write().await.insert(llm_proxy_pubkey);

        // 5. Store the CA certificate PEM string
        info!("NodeClient: Storing LLM Proxy CA certificate PEM.\nPEM:\n{}", payload.ca_cert_pem);
        let ca_cert = reqwest::Certificate::from_pem(payload.ca_cert_pem.as_bytes())?;
        let _ = self.llm_proxy_ca_cert.write().await.insert(ca_cert);
        info!("NodeClient: Successfully stored LLM Proxy CA certificate PEM.");

        println!("Stored LLM Proxy public key: {:?}", &self.llm_proxy_public_key.read().await);
        println!("Stored LLM Proxy CA certificate PEM: {:?}", &self.llm_proxy_ca_cert.read().await);

        let pubkey_log = self.llm_proxy_public_key.read().await.clone();
        let ca_cert_pem_log = self.llm_proxy_ca_cert.read().await.clone(); // Log the PEM string
        let pubkey_log_str = format!("{:?}", pubkey_log);
        let ca_cert_pem_log_str = format!("{:?}", ca_cert_pem_log); // Debug format of Option<String>

        Ok(serde_json::json!({
            "llm_proxy_public_key": pubkey_log_str,
            "llm_proxy_ca_cert_pem": ca_cert_pem_log_str, // Updated field name in log
        }).to_string())
    }

    /// Starts the llm-proxy service using Docker Compose, injecting the p2p-node's public key and RPC URL.
    pub async fn start_docker_service(
        &self,
        p2p_node_rpc_url: String,
        docker_compose_full_cmd_str: String,
    ) -> Result<()> {
        info!("Attempting to start llm-proxy service with command: {}", docker_compose_full_cmd_str);

        // Parse the docker_compose_full_cmd_str to extract file path and additional arguments
        let (
            docker_compose_file_path_str,
            additional_compose_args
        ) = parse_docker_compose_command(docker_compose_full_cmd_str)?;

        let docker_compose_path = std::path::Path::new(&docker_compose_file_path_str);
        debug!("Parsed docker compose file path: {:?}", docker_compose_path);
        debug!("Parsed additional compose args: {:?}", additional_compose_args);

        let p2p_node_pubkey_pem = encode_libp2p_pubkey_to_pem(&self.node_id.id_keys)
            .map_err(|e| anyhow!("Failed to encode p2p-node pubkey to PEM: {}", e))?;
        println!("NodeClient: p2p-node's Ed25519 public key PEM generated for llm-proxy.");

        if !docker_compose_path.exists() {
            return Err(anyhow!("Docker Compose file not found at: {:?}", docker_compose_path));
        }

        let compose_dir = docker_compose_path.parent().ok_or_else(|| {
            anyhow!("Could not determine directory of compose file: {:?}", docker_compose_path)
        })?;

        // Construct the full list of arguments for docker compose
        let mut final_compose_args: Vec<String> = vec![
            "compose".to_string(),
            "-f".to_string(),
            docker_compose_file_path_str.clone(), // Use the parsed file path
        ];
        final_compose_args.extend(additional_compose_args); // Add other parsed args (e.g., "up", "-d")

        info!("Running Docker command in dir {:?}: docker {:?}",
            compose_dir,
            final_compose_args,
        );

        let mut command = Command::new("docker");
        command
            .args(&final_compose_args) // Use the constructed arguments
            .current_dir(compose_dir)
            .env("P2P_NODE_PUBKEY", p2p_node_pubkey_pem)
            .env("P2P_NODE_RPC_URL", p2p_node_rpc_url)
            .stdout(std::process::Stdio::piped())
            .stderr(std::process::Stdio::piped());

        match command.spawn() {
            Ok(child) => {
                info!("llm-proxy service starting via docker-compose (PID: {})...", child.id());
                let output = tokio::task::spawn_blocking(move || {
                    child.wait_with_output()
                }).await??;

                 if output.status.success() {
                     info!("Docker Compose 'up' command completed successfully.");
                     debug!("stdout: {}", String::from_utf8_lossy(&output.stdout));
                     debug!("stderr: {}", String::from_utf8_lossy(&output.stderr));
                     Ok(())
                 } else {
                     error!("Docker Compose 'up' command failed with status: {}", output.status);
                     error!("stderr: {}", String::from_utf8_lossy(&output.stderr));
                     error!("stdout: {}", String::from_utf8_lossy(&output.stdout));
                     Err(anyhow!("Docker Compose command failed"))
                 }
            }
            Err(e) => {
                error!("Failed to execute docker-compose command: {}", e);
                Err(anyhow!("Failed to run docker-compose: {}. Is Docker running and docker-compose installed?", e))
            }
        }
    }

    pub async fn get_proxy_public_key(&self) -> Result<(Option<String>, Option<String>)> {
        let proxy_public_key = self.llm_proxy_public_key.read().await;
        let opt_public_key = proxy_public_key.as_ref();
        let public_key_str = match opt_public_key {
            Some(ref key) => Some(format!("{:?}", key)),
            None => None
        };

        let ca_cert = self.llm_proxy_ca_cert.read().await;
        let opt_ca_cert = ca_cert.as_ref();
        let ca_cert_str = match opt_ca_cert {
            Some(ref cert) => Some(format!("{:?}", cert)),
            None => None
        };
        Ok((public_key_str, ca_cert_str))
    }
}

// Helper function to parse the docker compose command string
// Expected format: "docker compose -f <file_path> <other_args>"
fn parse_docker_compose_command(command_str: String) -> Result<(String, Vec<String>), Error> {
    // Regex to capture:
    // - <file_path>: one or more non-whitespace characters after "-f "
    // - <compose_args>: any characters after the file_path, which will be split by whitespace
    let re = Regex::new(r"^docker\s*(?:-)?compose\s+-f\s+(?P<file_path>\S+)(?P<args_group>\s+.*)?$")
        .map_err(|e| anyhow!("Failed to compile regex for docker command parsing: {}", e))?;

    if let Some(caps) = re.captures(&command_str) {
        let file_path = caps.name("file_path")
            .expect("file_path must be captured if regex matches") // This part is not optional in the regex
            .as_str()
            .to_string();

        let compose_args_str = caps.name("args_group") // This group is optional: (?P<args_group>\s+.*)?
            .map_or("", |m| m.as_str().trim_start()); // If present, trim leading space(s) captured by \s+

        // Split the remaining arguments by whitespace.
        // Filter out empty strings that might result from multiple spaces (though split_whitespace handles this).
        let compose_args_vec = compose_args_str
            .split_whitespace()
            .map(String::from)
            .collect::<Vec<String>>();

        Ok((file_path, compose_args_vec))
    } else {
        Err(anyhow!(
            "Command '{}' does not match expected format 'docker-compose -f <file_path> <args>' or 'docker compose -f <file_path> <args>'",
            command_str
        ))
    }
}


#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_parse_docker_compose_command_valid_simple() {
        let cmd = "docker compose -f /path/to/docker-compose.yml up -d".to_string();
        let (file_path, args) = parse_docker_compose_command(cmd).unwrap();
        assert_eq!(file_path, "/path/to/docker-compose.yml");
        assert_eq!(args, vec!["up".to_string(), "-d".to_string()]);
    }

    #[test]
    fn test_parse_docker_compose_command_valid_simple_hyphenated() {
        let cmd = "docker-compose -f /path/to/docker-compose.yml up -d".to_string();
        let (file_path, args) = parse_docker_compose_command(cmd).unwrap();
        assert_eq!(file_path, "/path/to/docker-compose.yml");
        assert_eq!(args, vec!["up".to_string(), "-d".to_string()]);
    }

    #[test]
    fn test_parse_docker_compose_command_extra_whitespace_around_f() {
        let cmd = "docker compose   -f   /path/to/docker-compose.yml up -d".to_string();
        let (file_path, args) = parse_docker_compose_command(cmd).unwrap();
        assert_eq!(file_path, "/path/to/docker-compose.yml");
        assert_eq!(args, vec!["up".to_string(), "-d".to_string()]);

        let cmd_hyphen = "docker-compose   -f   /path/to/docker-compose.yml up -d".to_string();
        let (file_path_h, args_h) = parse_docker_compose_command(cmd_hyphen).unwrap();
        assert_eq!(file_path_h, "/path/to/docker-compose.yml");
        assert_eq!(args_h, vec!["up".to_string(), "-d".to_string()]);
    }

    #[test]
    fn test_parse_docker_compose_command_extra_whitespace_before_args() {
        let cmd = "docker compose -f /path/to/docker-compose.yml   up -d".to_string();
        let (file_path, args) = parse_docker_compose_command(cmd).unwrap();
        assert_eq!(file_path, "/path/to/docker-compose.yml");
        assert_eq!(args, vec!["up".to_string(), "-d".to_string()]);
    }

    #[test]
    fn test_parse_docker_compose_command_no_compose_args() {
        let cmd = "docker compose -f /path/to/docker-compose.yml".to_string();
        let (file_path, args) = parse_docker_compose_command(cmd).unwrap();
        assert_eq!(file_path, "/path/to/docker-compose.yml");
        assert!(args.is_empty());

        let cmd_trailing_space = "docker compose -f /path/to/docker-compose.yml  ".to_string();
        let (file_path_ts, args_ts) = parse_docker_compose_command(cmd_trailing_space).unwrap();
        assert_eq!(file_path_ts, "/path/to/docker-compose.yml");
        assert!(args_ts.is_empty());
    }

    #[test]
    fn test_parse_docker_compose_command_single_compose_arg() {
        let cmd = "docker-compose -f ./file.yml build".to_string();
        let (file_path, args) = parse_docker_compose_command(cmd).unwrap();
        assert_eq!(file_path, "./file.yml");
        assert_eq!(args, vec!["build".to_string()]);
    }

    #[test]
    fn test_parse_docker_compose_command_args_with_hyphens_and_paths() {
        let cmd = "docker compose -f ../relative/path/test.yaml logs -f --tail=100 service_name".to_string();
        let (file_path, args) = parse_docker_compose_command(cmd).unwrap();
        assert_eq!(file_path, "../relative/path/test.yaml");
        assert_eq!(args, vec!["logs".to_string(), "-f".to_string(), "--tail=100".to_string(), "service_name".to_string()]);
    }

    #[test]
    fn test_parse_docker_compose_command_invalid_no_f_flag() {
        let cmd = "docker compose /path/to/docker-compose.yml up -d".to_string();
        assert!(parse_docker_compose_command(cmd).is_err());
    }

    #[test]
    fn test_parse_docker_compose_command_invalid_wrong_prefix() {
        let cmd = "mydocker compose -f /path/to/docker-compose.yml up -d".to_string();
        assert!(parse_docker_compose_command(cmd).is_err());
    }

    #[test]
    fn test_parse_docker_compose_command_empty_string() {
        let cmd = "".to_string();
        assert!(parse_docker_compose_command(cmd).is_err());
    }

    #[test]
    fn test_parse_docker_compose_command_just_prefix() {
        let cmd = "docker compose -f".to_string();
        assert!(parse_docker_compose_command(cmd).is_err());

        let cmd2 = "docker-compose -f ".to_string();
        assert!(parse_docker_compose_command(cmd2).is_err());
    }
}

