services:
  # The proxy service intercepts and logs all LLM API calls
  llm-proxy:
    container_name: llm-proxy
    build:
      context: . # Build context is the project root
      dockerfile: llm-proxy/Dockerfile # Path relative to context
    image: llm-proxy-local # Optional: name the image
    restart: unless-stopped
    environment:
      - RUST_LOG=info,llm_proxy=debug
      - REPORT_USAGE_URL=http://host.docker.internal:8002
      - LLM_PROXY_INTERNAL_PORT=7070 # Set the internal API port
      - CA_CERT_PATH=/certs_src/hudsucker.cer
    volumes:
      # Mount host CA certs for runtime reading/writing of hudsucker certs
      - ./llm-proxy/certs:/certs_src
      # Mount llm-proxy directory for receiving p2p-node public key
      - ./llm-proxy/pubkeys/llm-proxy:/pubkeys/llm-proxy # Needs RW
      # Mount p2p-node directory for llm-proxy to read
      - ./llm-proxy/pubkeys/p2p-node:/pubkeys/p2p-node:ro
    networks:
      - llm-network  # Connects to python-llm-server
      - default      # Also connects to the default bridge network (which has external access)
    # Expose port only if direct external access needed
    ports:
      - "8080:8080"
      - "7070:7070"

  # Caddy v2 Reverse proxy forwards inbound requests to python-llm-server
  caddy:
    image: caddy:latest # Use official Caddy image
    container_name: caddy
    restart: unless-stopped
    ports:
      - "8000:80" # Map host 8000 to Caddy's internal port 80 (defined in Caddyfile)
    volumes:
      # Mount the Caddyfile (adjust host path as needed)
      - ./llm-proxy/caddy/Caddyfile:/etc/caddy/Caddyfile
    networks:
      - llm-network # Can reach python-llm-server
      - default     # Can be reached from host

  #########################################################
  ## Dynamically deployed Python LLM agents
  #########################################################

  # The Python LLM server that makes API calls
  python-llm-server:
    build:
      context: ./agents/python/execute_with_memories
      dockerfile: Dockerfile
    environment:
      # Route all HTTP and HTTPS traffic through our proxy
      - HTTP_PROXY=http://llm-proxy:8080
      - HTTPS_PROXY=http://llm-proxy:8080
      # Don't proxy requests to localhost or the proxy itself
      - NO_PROXY=localhost,127.0.0.1,llm-proxy
      # Point requests library to the proxy's CA certificate
      - REQUESTS_CA_BUNDLE=/certs/hudsucker.cer
      # Set SSL environment variables for Python
      - SSL_CERT_FILE=/certs/hudsucker.cer
    volumes:
      - ./llm-proxy/certs:/certs:ro
    networks:
      - llm-network  # Connects to llm-proxy only
    depends_on:
      - llm-proxy
      - caddy
    restart: unless-stopped
    command: >
      sh -c "
        echo 'Waiting for proxy CA certificate...'
        while [ ! -f /certs/hudsucker.cer ]; do
          echo 'CA certificate not found, waiting...'
          sleep 1
        done
        echo 'Found CA certificate!'
        echo 'Certificate details:'
        openssl x509 -in /certs/hudsucker.cer -text -noout | grep -E 'Subject:|DNS:|Issuer:|Validity'
        echo 'Starting Python server...'
        python main.py
      "

networks:
  llm-network:
    driver: bridge
    internal: true
    # `internal: true` restricts external network access.
    # Restriction applies to all IP traffic originating from containers attached
    # exclusively to this network, regardless of transport protocol (TCP, UDP)
    # or application protocol (HTTP, HTTPS, gRPC, WebSockets, raw sockets, etc.)