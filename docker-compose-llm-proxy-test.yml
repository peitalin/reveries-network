services:
  # The proxy service intercepts and logs all LLM API calls
  llm-proxy:
    container_name: llm-proxy
    build:
      context: . # Build context is the project root
      dockerfile: llm-proxy/Dockerfile # Path relative to context
    image: llm-proxy-local # Optional: name the image
    restart: unless-stopped
    environment:
      - RUST_LOG=info,llm_proxy=debug
      - REPORT_USAGE_URL=http://host.docker.internal:8002
      - LLM_PROXY_INTERNAL_PORT=7070 # Set the internal API port
      # Test environment variables
      - TEST_API=true
      - NODE_PUBKEY_PATH=/pubkeys/p2p-node/p2p_node.pub.pem
      - NODE_TEST_PUBKEY_PATH=/pubkeys/p2p-node/p2p_node_test.pub.pem
      - CA_CERT_PATH=/certs_src/hudsucker.cer
    volumes:
      # Mount host CA certs for runtime reading/writing of hudsucker certs
      - ./llm-proxy/certs:/certs_src
      # Mount llm-proxy directory for receiving p2p-node public key
      - ./llm-proxy/pubkeys/llm-proxy:/pubkeys/llm-proxy # Needs RW
      # Mount p2p-node directory for llm-proxy to read
      - ./llm-proxy/pubkeys/p2p-node:/pubkeys/p2p-node:ro
    networks:
      # Network dynamic python servers will join
      - llm-network
    # Expose port only if direct external access needed (maybe not)
    ports:
      - "8080:8080"
      - "7070:7070"

networks:
  llm-network:
    driver: bridge
    # Name can be customized if needed