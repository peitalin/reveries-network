services:
  # The proxy service intercepts and logs all LLM API calls
  llm-proxy:
    container_name: llm-proxy
    build:
      context: . # Build context is the project root
      dockerfile: llm-proxy/Dockerfile # Path relative to context
    image: llm-proxy-local # Optional: name the image
    restart: unless-stopped
    environment:
      - RUST_LOG=info,llm_proxy=debug
      ### NOTE: hardcoded to node2's port:
      - REPORT_USAGE_URL=http://host.docker.internal:9902
      - INTERNAL_API_KEY_SERVER_PORT=7070 # Set the internal API port
      - CA_CERT_PATH=/certs_src/hudsucker.cer
      ## API key server
      - TEST_API=true
      - P2P_NODE_PUBKEY=${P2P_NODE_PUBKEY} # Injected by p2p-node start command
      - P2P_NODE_RPC_URL=${P2P_NODE_RPC_URL} # Needs to be set in p2p-node's env or passed
    volumes:
      # Mount host CA certs for runtime reading/writing of hudsucker certs
      - ./llm-proxy/certs:/certs_src
      # Mount llm-proxy directory for receiving p2p-node public key
      # - ./llm-proxy/pubkeys/llm-proxy:/pubkeys/llm-proxy # Needs RW
      # # Mount p2p-node directory for llm-proxy to read
      # - ./llm-proxy/pubkeys/p2p-node:/pubkeys/p2p-node:ro
    networks:
      - llm-network  # Connects to python-llm-server
      - default      # Also connects to the default bridge network (which has external access)
    # Expose port only if direct external access needed
    ports:
      - "7666:7666"
      - "7070:7070"

  # # Caddy v2 Reverse proxy forwards inbound requests to python-llm-server
  # caddy:
  #   image: caddy:latest # Use official Caddy image
  #   container_name: caddy
  #   restart: unless-stopped
  #   ports:
  #     - "6000:80" # Map host 6000 to Caddy's internal port 80 (defined in Caddyfile)
  #   volumes:
  #     # Mount the Caddyfile (adjust host path as needed)
  #     - ./llm-proxy/caddy/Caddyfile:/etc/caddy/Caddyfile
  #   networks:
  #     - llm-network # Can reach python-llm-server
  #     - default     # Can be reached from host

  # ### TODO: configure router + port forwarding for
  # ### NEAR inbound ports 24567
  # nearcore-arm64:
  #   # image: nearprotocol/nearcore:latest
  #   image: nearcore-arm64
  #   container_name: nearcore-arm64
  #   build:
  #     context: . # Build context is the project root
  #     dockerfile: Dockerfile-neard # Path relative to context
  #   restart: unless-stopped
  #   volumes:
  #     - near_data_volume:/root/.near
  #   networks:
  #     - default     # Can be reached from host
  #   ports:
  #     - "3030:3030"  # RPC port
  #     - "24567:24567"  # Network port
  #   deploy: # Requires Docker Compose V2+
  #     resources:
  #       limits:
  #         memory: 24G # hits 20.5gb of memory
  #       reservations:
  #         memory: 8G
  #   memswap_limit: 28g

  # #########################################################
  # ## Dynamically deployed Python LLM agents
  # #########################################################

  # The Python LLM server that makes API calls
  python-llm-server:
    build:
      context: ./agents/python/execute_with_memories
      dockerfile: Dockerfile
    environment:
      # Route all HTTP and HTTPS traffic through our proxy
      - HTTP_PROXY=http://llm-proxy:7666
      - HTTPS_PROXY=http://llm-proxy:7666
      # Don't proxy requests to localhost or the proxy itself
      - NO_PROXY=localhost,127.0.0.1,llm-proxy,reverse-proxy,weather-mcp
      # - NO_PROXY=localhost,127.0.0.1,llm-proxy,reverse-proxy,weather-mcp,weather-mcp:8000,weather-mcp:8000/sse
      # Point requests library to the proxy's CA certificate
      - REQUESTS_CA_BUNDLE=/certs/hudsucker.cer
      # Set SSL environment variables for Python
      - SSL_CERT_FILE=/certs/hudsucker.cer
      # Dummy API key triggers API Key delegation (Anthropic SDK requires an API key)
      - ANTHROPIC_API_KEY=sk-ant-delegated-api-key
      # Weather service configuration - updated with correct SSE path
      - WEATHER_MCP_URL=http://weather-mcp:8000/sse
    volumes:
      - ./llm-proxy/certs:/certs:ro
    networks:
      - llm-network  # Connects to llm-proxy only
    depends_on:
      - llm-proxy
    restart: unless-stopped
    command: >
      sh -c "
        echo 'Waiting for proxy CA certificate...'
        while [ ! -f /certs/hudsucker.cer ]; do
          echo 'CA certificate not found, waiting...'
          sleep 1
        done
        echo 'Found CA certificate!'
        echo 'Certificate details:'
        openssl x509 -in /certs/hudsucker.cer -text -noout | grep -E 'Subject:|DNS:|Issuer:|Validity'
        echo 'Starting Python server...'
        python main.py
      "

  weather-mcp:
    build:
      context: ./agents/mcp/weather-server-python
      dockerfile: Dockerfile
    container_name: weather-mcp
    restart: always
    networks:
      - llm-network
    ports:
      - "8000:8000" # default FastAPI port
    command: python weather.py

networks:
  llm-network:
    driver: bridge
    internal: false
    # `internal: true` restricts external network access.
    # Restriction applies to all IP traffic originating from containers attached
    # exclusively to this network, regardless of transport protocol (TCP, UDP)
    # or application protocol (HTTP, HTTPS, gRPC, WebSockets, raw sockets, etc.)

# Declare the named volume at the top level
volumes:
  near_data_volume:
    driver: local # Optional: specify driver (local is default)