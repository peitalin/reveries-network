services:
  # The proxy service intercepts and logs all LLM API calls
  llm-proxy:
    build:
      context: .
      dockerfile: llm-proxy/Dockerfile
    ports:
      - "8080:8080"
    volumes:
      - ./logs:/logs
      - ./llm-proxy/certs:/certs
    networks:
      - llm-network
    restart: unless-stopped

  # The Python LLM server that makes API calls
  python-llm-server:
    build:
      context: ./agents/python/execute_with_memories
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      # Route all HTTP and HTTPS traffic through our proxy
      - HTTP_PROXY=http://llm-proxy:8080
      - HTTPS_PROXY=http://llm-proxy:8080
      # Don't proxy requests to localhost or the proxy itself
      - NO_PROXY=localhost,127.0.0.1,llm-proxy
      # Point requests library to the proxy's CA certificate
      - REQUESTS_CA_BUNDLE=/certs/hudsucker.cer
      # Set SSL environment variables for Python
      - SSL_CERT_FILE=/certs/hudsucker.cer
    volumes:
      - ./llm-proxy/certs:/certs:ro
    networks:
      - llm-network
    depends_on:
      - llm-proxy
    restart: unless-stopped
    command: >
      sh -c "
        echo 'Waiting for proxy CA certificate...'
        while [ ! -f /certs/hudsucker.cer ]; do
          echo 'CA certificate not found, waiting...'
          sleep 1
        done
        echo 'Found CA certificate!'
        echo 'Certificate details:'
        openssl x509 -in /certs/hudsucker.cer -text -noout | grep -E 'Subject:|DNS:|Issuer:|Validity'
        echo 'Starting Python server...'
        python main.py
      "

volumes:
  certs:
    driver: local

networks:
  llm-network:
    driver: bridge